{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape!\n",
    "---\n",
    "\n",
    "This notebook scrapes the following info for a specific number of games from the Steam store and adds it to our existing database of game records, currently saved in /data/raw/0 - Scraped Games DF.json:\n",
    "\n",
    "------\n",
    "\n",
    "'app_id' <-- The unique ID assigned to each game by the Steam store (int)\n",
    "\n",
    "'title' <-- Game's title\n",
    "\n",
    "'release_date' <-- Game's release date\n",
    "\n",
    "'positive_review_percent' <-- Self-explanatory\n",
    "\n",
    "'number_of_reviews' <-- Number of reviews in ALL languages\n",
    "\n",
    "'price' <-- In cents (so a $10.00 game will be 1000.0)\n",
    "\n",
    "'game_page_link' <-- Link to that game's Steam store page\n",
    "\n",
    "'tags' <-- Numeric representation of the top 7 (max) descriptive tags assigned by users of the Steam store\n",
    "\n",
    "'tag_list' <-- List of strings holding the top 20 (max) descriptive tags assigned by users of the Steam store\n",
    "\n",
    "'date_scraped' <-- Date that this info was added to the db\n",
    "\n",
    "'developer' <-- Game's developer(s)\n",
    "\n",
    "'publisher' <-- Game's publisher(s)\n",
    "\n",
    "'description' <-- The little blurb\n",
    "\n",
    "'interface_languages' <-- Languages with full interface support\n",
    "\n",
    "'full_audio_languages' <-- Languages with full audio support\n",
    "\n",
    "'subtitles_languages' <-- Languages with full subtitle support\n",
    "\n",
    "'english' <-- This and all following columns contain the number of comments in that language\n",
    "\n",
    "'schinese', 'tchinese', 'japanese', 'koreana', 'thai', 'bulgarian', 'czech', 'danish', 'german', 'spanish', 'latam', 'greek', 'french', 'italian', 'indonesian', 'hungarian', 'dutch', 'norwegian', 'polish', 'brazilian', 'romanian', 'russian', 'finnish', 'swedish', 'turkish', 'vietnamese'\n",
    "\n",
    "-------\n",
    "\n",
    "Other notebooks will further transform this data in preparation for modeling.\n",
    "\n",
    "The number of games to scrape is in the second executable cell, and can be set one each run.\n",
    "\n",
    "IMPORTANT NOTE: This notebook only functions if the Steam store is loading WITHOUT infinite scroll. Thus far, I have been able to make sure that it doesn't use infinite scroll by logging into my Steam account from a browser (Chrome) and unchecking the \"Enable infinite scroll when searching\" box under \"Store Preferences.\" IF YOU HAVEN'T LOGGED IN IN A WHILE, THIS SETTING MAY REVERT! So to be sure, check your settings before each run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DS stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Trying not to get blocked while scraping by inputting\n",
    "# random delays between Get requests.\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# I needed some extra help locating specific parts within a\n",
    "# bs4 tag object, so I got this.\n",
    "import re\n",
    "\n",
    "# For labeling records, tracking files, and formatting\n",
    "from datetime import datetime\n",
    "\n",
    "# To help see if we have existing data or not.\n",
    "import os\n",
    "\n",
    "# For Rick\n",
    "import pickle\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable will determine how many games to scrape per notebook execution.\n",
    "# I'm putting it all the way up here to make it easier to find & modify.\n",
    "%store -r interval\n",
    "\n",
    "games_to_scrape = interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Learn about the page\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: THIS CODE ONLY WORKS IF YOUR STEAM SETTINGS ARE SET TO PAGINATED\n",
    "# SEARCH RESULTS, NOT INFINITE SCROLL.\n",
    "# It works for me if I log into the Steam store on a browser (Chrome),\n",
    "# go to \"Store Preferences,\" then uncheck the \"Enable infinite scroll\n",
    "# when searching\" box.\n",
    "# That setting apparently applies to all requests made by this notebook\n",
    "# as well.\n",
    "\n",
    "# This url is for the \"all products\" search with the result type\n",
    "# limited to \"Games\" (category1=998)\n",
    "url = \"https://store.steampowered.com/search/?category1=998\"\n",
    "html = urlopen(url)\n",
    "current_page_soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Scrape the first set of data from the search results pages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we know what our soups will look like, we can write functions to do the scraping.\n",
    "# The first function will scrape all the relevant data off of the current results page.\n",
    "# The second function will programmatically switch to the next page of results.\n",
    "# Later, we will run both functions within a loop in order to scrape all results data\n",
    "# from all pages.\n",
    "\n",
    "# This is only the first round of scraping. Later, we will scrape more data from each\n",
    "# game's store page. Since that process is completely different, we will define new\n",
    "# functions for it later, after this round of scraping is complete.\n",
    "\n",
    "# Loop through the HTML blocks for each game and scrape the key info into a dictionary,\n",
    "# then add the dictionaries to the list.\n",
    "# I'm not cleaning up the data types at this point - I'm learning as I'm going, so I'm\n",
    "# prioritizing getting all the info I need into the df, and then working with data\n",
    "# types later either by doing operations on the df or re-writing some of this code.\n",
    "def scrape_current_page(current_page_soup) :\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes the soup of a paginated Steam search results page (NOT infinte scroll)\n",
    "    and scrapes the:\n",
    "    \n",
    "    title\n",
    "    release_date\n",
    "    positive_review_percent\n",
    "    number_of_reviews\n",
    "    price\n",
    "    game_page_link\n",
    "    type\n",
    "    app_id\n",
    "    \n",
    "    from every game on the page. It puts these values into dictionaries and appends them to\n",
    "    the list called \"games\". \n",
    "    \"\"\"\n",
    "\n",
    "    for listing in current_page_soup.find_all('a', class_='search_result_row ds_collapse_flag') :\n",
    "\n",
    "        # Create (or clean out) an empty dictionary to hold the new info.\n",
    "        game = {}\n",
    "\n",
    "        # Check if we have the specified number of games yet.\n",
    "        if len(games) == games_to_scrape :\n",
    "            return\n",
    "\n",
    "        # Listings on results pages can be one of two types - standalone games, or bundles.\n",
    "        # We only want to work with standalone games.\n",
    "        # Only apps have this tag in their listing.\n",
    "        if listing.has_attr('data-ds-appid') :\n",
    "            raw_app_id = listing.get('data-ds-appid')\n",
    "            # To exclude bundles, we'll skip any listing with multiple app_ids.\n",
    "            # Since the app_id is scraped as a string, lists of app_ids will have\n",
    "            # commas separating them, and we can identify them by those commas.\n",
    "            if \",\" in raw_app_id :\n",
    "                continue\n",
    "            \n",
    "            app_id = int(raw_app_id)\n",
    "\n",
    "            # Make sure we haven't already scraped this one.\n",
    "            if app_id not in already_scraped_app_ids.values :\n",
    "                \n",
    "                game['app_id'] = app_id\n",
    "                    \n",
    "                # The title and release date seem to be at uniform locations in all listings.\n",
    "                game['title'] = listing.find('span', class_='title').get_text()\n",
    "                raw_date = listing.find('div', class_='col search_released responsive_secondrow').get_text()\n",
    "                raw_date = raw_date.strip()\n",
    "                try:\n",
    "                    formatted_date = datetime.strptime(raw_date, '%b %d, %Y')\n",
    "                    game['release_date'] = formatted_date\n",
    "                except:\n",
    "                    try: \n",
    "                        formatted_date = datetime.strptime(raw_date, '%b %Y')\n",
    "                        game['release_date'] = formatted_date\n",
    "                    except :\n",
    "                        game['release_date'] = raw_date\n",
    "\n",
    "                # Not all games have reviws listed, so we have to account for code blocks that omit this part.\n",
    "                # I might eventually remove this part and scrape the review data from the individual game pages\n",
    "                # instead, since it seems to be more complete there. This is just proof of concept for now.\n",
    "                try:\n",
    "                    review_string = re.split('>| of|the | user', listing.find('div', class_='col search_reviewscore responsive_secondrow') \\\n",
    "                                                                .find('span').get('data-tooltip-html'))\n",
    "                    raw_review_percent = review_string[1][:-1]\n",
    "                    float_review_percent = int(raw_review_percent) / 100\n",
    "                    formatted_review_percent = round(float_review_percent, 2)\n",
    "                    game['positive_review_percent'] = formatted_review_percent\n",
    "                except:\n",
    "                    game['positive_review_percent'] = np.nan\n",
    "                \n",
    "                try: \n",
    "                    review_string = re.split('>| of|the | user', listing.find('div', class_='col search_reviewscore responsive_secondrow') \\\n",
    "                                                .find('span').get('data-tooltip-html'))\n",
    "                    raw_review_number = review_string[3].replace(',', '')\n",
    "                    formatted_review_number = int(raw_review_number)\n",
    "                    game['number_of_reviews'] = formatted_review_number\n",
    "                except: \n",
    "                    game['number_of_reviews'] = np.nan\n",
    "                \n",
    "                # Same for price - many unreleased games do not have price info, so we have to skip them.\n",
    "                # Some games have an original price and a discounted price listed, but for the time being\n",
    "                # I've decided to only go by original prices, so I'll default to that and only return\n",
    "                # a null value if no kind of price whatsoever is listed.\n",
    "                try: \n",
    "                    raw_price = listing.find('div', class_=\"discount_original_price\").get_text()\n",
    "                    bare_price = raw_price.replace(\"$\", \"\").replace(\",\", \"\").replace(\".\", \"\")\n",
    "                    formatted_price = int(bare_price)\n",
    "                    game['price'] = formatted_price\n",
    "                except:\n",
    "                    try:\n",
    "                        raw_price = listing.find('div', class_=\"discount_final_price\").get_text()\n",
    "                        bare_price = raw_price.replace(\"$\", \"\").replace(\",\", \"\").replace(\".\", \"\")\n",
    "                        formatted_price = int(bare_price)\n",
    "                        game['price'] = formatted_price\n",
    "                    except:\n",
    "                        try:\n",
    "                            raw_price = listing.find('div', class_=\"discount_final_price free\").get_text()\n",
    "                            formatted_price = 0\n",
    "                            game['price'] = formatted_price\n",
    "                        except:\n",
    "                            game['price'] = np.nan\n",
    "\n",
    "                # Weirdly enough, not every game seems to have its own page.\n",
    "                try:\n",
    "                    game['game_page_link'] = listing.get('href')\n",
    "                except:\n",
    "                    game['game_page_link'] = 'Failed'\n",
    "\n",
    "                # Now we grab the tags, which will be a major feature in our analysis.\n",
    "                try :\n",
    "                    raw_tags = listing.get('data-ds-tagids')\n",
    "                    formatted_tags = raw_tags.strip('[]').split(',')\n",
    "                    game['tags'] = formatted_tags\n",
    "                except :\n",
    "                    game['tags'] = 'Failed'\n",
    "\n",
    "                # Add the current date as a reference for future generations (and versioning).\n",
    "                todays_date = datetime.now()\n",
    "                game['date_scraped'] = todays_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                # Now we add this dict to the list, rinse and repeat.\n",
    "                games.append(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the function that determines if there is a next page of\n",
    "# results, or if we're already at the last page.\n",
    "\n",
    "def get_next_page_url(current_page_soup) :\n",
    "\n",
    "        \"\"\"\n",
    "        This function takes the soup of a paginated Steam search results page (NOT infinte scroll)\n",
    "        and determines whether it is the last page of results.\n",
    "\n",
    "        If it is not the last page, the URL of the next page is stored in \"next_link\".\n",
    "\n",
    "        If it is the last page, \"next_link\" will be set to False.\n",
    "        \"\"\"\n",
    "\n",
    "        # First, we check to make sure there IS a next page. We can tell by looking\n",
    "        # at the 'pagebtn' tags.\n",
    "        pagebtn_tags = current_page_soup.find_all('a', class_='pagebtn')\n",
    "\n",
    "        # This is the variable that we will use to store the next link, or set it to\n",
    "        # False to let the loop know that we're done scraping.\n",
    "        global next_link\n",
    "\n",
    "        # If it is any of the middle pages, there will be two pagebtn tags.\n",
    "        # The link we need is in side the pagebtn tag that displays the text '>'.\n",
    "\n",
    "        # After a while this loop fails because we get an empty pagebtn tag, which\n",
    "        # means that the page failed to load altogether. I will assume this is because\n",
    "        # the request timed out, and build in an additional delay to deal with that\n",
    "        # eventuality.\n",
    "        loops = 0\n",
    "        delay = 0\n",
    "\n",
    "        while loops < 5 :\n",
    "                try :\n",
    "                        if len(pagebtn_tags) == 2 :\n",
    "                                next_link = pagebtn_tags[1].get('href')\n",
    "\n",
    "                        # If there is only one pagebtn tag, that means we're on the first page or the \n",
    "                        # last page. If it's the first page, then the pagebtn tag will contain the\n",
    "                        # character '>'.\n",
    "                        elif pagebtn_tags[0].get_text() == \">\" :\n",
    "                                next_link = pagebtn_tags[0].get('href')\n",
    "\n",
    "                        # If neither of the above conditions are met, then we're on the last page and\n",
    "                        # we can set \"next_link\" to False, triggering the loop to stop scraping.\n",
    "                        else :\n",
    "                                next_link = False\n",
    "\n",
    "                        # We will use loops=100 as the indicator that the scrape was successful.\n",
    "                        # This in no way means that 100 loops actually having happened.\n",
    "                        # It's just a random value that can't (or shouldn't) occur naturally.\n",
    "                        loops = 100\n",
    "\n",
    "                except :\n",
    "                        # If the scrape was unsuccessful, we will try again 4 more times, with\n",
    "                        # an increasingly log delay between each attempt.\n",
    "                        delay += 10\n",
    "                        print(\"Search page failure \"+str(delay//10)+\"/5...\")\n",
    "                        print(next_link)\n",
    "                        time.sleep(delay)\n",
    "                        loops += 1\n",
    "        \n",
    "        if loops != 100 :\n",
    "                print('Failed to parse next_link on search results page:')\n",
    "                print(next_link)\n",
    "                next_link = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 17503 existing game records.\n",
      "Stored 'next_link' (str)\n",
      "2 games scraped from search page.\n"
     ]
    }
   ],
   "source": [
    "# Now that we have our functions, we'll iterate over them to scrape the data.\n",
    "\n",
    "# Set the first url to be processed to the first page of search results...\n",
    "# OR to a couple pages before the last-scraped search results page from the previous\n",
    "# notebook run.\n",
    "try :\n",
    "    %store -r next_link\n",
    "except :\n",
    "    next_link = url\n",
    "\n",
    "# Create the list that will hold the dictionaries of game info & errors.\n",
    "games = []\n",
    "\n",
    "# Now we decide how many results we want. \n",
    "# \n",
    "# The main constraint here is time - since\n",
    "# we don't want to get IP banned, we'll have set delay between each get request.\n",
    "# This isn't so important for this loop, since we can get 25 games in one get request.\n",
    "# However, later we'll be going through the games' pages one-by-one, and in some cases\n",
    "# we'll have to do 10 different get requests per game to scrape language-specific data.\n",
    "# Therefore, adding 1 game adds at least 11 get requests & delays to our process.\n",
    "# (I ended up scraping for over 10 hours.)\n",
    "#\n",
    "# Will only limit to inteverals of 25 (as there are 25 results per page).\n",
    "# If games_to_scrape is greater than the number of games in the search results, then\n",
    "# the the will automatically stop trying to scrape when it reaches the end of the\n",
    "# final page of results, because get_next_page_url will set the next_link variable to False.\n",
    "\n",
    "# Since we want to update this database from time to time, let's devise a way to scrape only\n",
    "# game data that is not already in the .json file that holds our master list. We'll do this\n",
    "# by pulling the app_ids from that file (if it exists) and pulling out only the app_ids to\n",
    "# check against.\n",
    "\n",
    "if os.path.exists('../data/raw/0 - Scraped Games DF.pkl') :\n",
    "    with open('../data/raw/0 - Scraped Games DF.pkl', 'rb') as file:\n",
    "        check_df = pickle.load(file)\n",
    "        already_scraped_app_ids = check_df['app_id']\n",
    "        print(f\"Identified {len(already_scraped_app_ids)} existing game records.\")\n",
    "else :\n",
    "    print('No scraped games detected. Scraping from scratch.')\n",
    "    already_scraped_app_ids = pd.Series()\n",
    "\n",
    "# Now, loop. Keep scraping as long as our games list is shorter than the games_to_scrape var.\n",
    "    \n",
    "while len(games) < games_to_scrape :\n",
    "\n",
    "    # Soup up the page in question.\n",
    "    html = urlopen(next_link)\n",
    "    current_page_soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # Scrape that page.\n",
    "    scrape_current_page(current_page_soup)\n",
    "\n",
    "    # We store the next_link variable after successful scrape but before updating\n",
    "    # because the scrape_current_page function escapes when reacing the total\n",
    "    # number of games_to_scrape, NOT specfically when all results on the page\n",
    "    # are scraped.\n",
    "    # Thus, if we want to start scraping again, we should start with the current\n",
    "    # page and make sure there aren't any leftover games there before updating\n",
    "    # next_link.\n",
    "    %store next_link\n",
    "\n",
    "    # Set \"next_link\" to the next URL we want to scrape.\n",
    "    get_next_page_url(current_page_soup)\n",
    "\n",
    "    # Include a random delay to prevent getting IP blocked.\n",
    "    interval = 0.5 + random.random() * 0.3\n",
    "    time.sleep(interval)\n",
    "\n",
    "    if next_link == False :\n",
    "        print('Fewer than '+str(games_to_scrape)+' games in the available search results.')\n",
    "        if len(games) == 0 :\n",
    "            raise UserWarning('No games scraped from search results. Cannot continue.')\n",
    "\n",
    "print(str(len(games))+' games scraped from search page.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame it and check.\n",
    "scraped_search_results_df = pd.DataFrame(games)\n",
    "\n",
    "# This results in some duplicates - sometimes different versions of the game have the same app id.\n",
    "# Because we're interested in the relative ration of comment frequencies, not in the total number\n",
    "# of games or total number of comments, we can safely drop duplicates even if they have different\n",
    "# comments.\n",
    "# Since we already excluded app_ids that are duplicates with our previously-scraped files, now we\n",
    "# must remove duplicates that might exist within that last scrape.\n",
    "scraped_search_results_df = scraped_search_results_df.drop_duplicates(subset='app_id', keep='first')\n",
    "scraped_search_results_df = scraped_search_results_df.reset_index(drop=True)\n",
    "\n",
    "# Save this as a json to safeguard against crashes - running this scraper takes hours.\n",
    "scraped_search_results_df.to_json('../data/raw/temp/Scraped Search Results.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Scrape additional data for each game from its individual game page\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're ready to use the URLs we just scraped to go through the pages\n",
    "# one-by-one and scrape more data.\n",
    "\n",
    "# We'll put all this data in a completely different df, then join them\n",
    "# when we're done on app_id.\n",
    "def scrape_game_page_data(current_page_soup, app_id) :\n",
    "\n",
    "    \"\"\"\n",
    "    This function scrapes info from all the individual games pages\n",
    "    currently referenced in games_info_df. We put the info in a dict\n",
    "    \"game\", then append it to \"games_extend_list\".\n",
    "    \n",
    "    Later, we will turn that list into another df and merge it to\n",
    "    games_info_df on index.\n",
    "\n",
    "    Scraped information is:\n",
    "\n",
    "    app_id\n",
    "    developer\n",
    "    publisher\n",
    "    description\n",
    "    interface_languages\n",
    "    full_audio_languages\n",
    "    subtitles_languages\n",
    "    english     <-- the number of user comments in English\n",
    "    \"\"\"\n",
    "    # For bugfixing\n",
    "    global touched_ids\n",
    "    \n",
    "    # Create/clear out the dictionary.\n",
    "    game = {}\n",
    "\n",
    "    game[\"app_id\"] = app_id\n",
    "    touched_ids.append(game['app_id'])\n",
    "\n",
    "    # We can get the developer and publisher from the same code block.\n",
    "    try :\n",
    "        code_block = current_page_soup.find('div', attrs={'id':'appHeaderGridContainer'})\n",
    "    except :\n",
    "        pass\n",
    "\n",
    "    # The developer name is at a fixed location.\n",
    "    try:\n",
    "        raw_string = code_block.find('div', class_='grid_content').get_text()\n",
    "        # Don't know why it always brings in a newline at the beginning of the string. and a\n",
    "        # space at the end. Let's take those out.\n",
    "        formatted_string = raw_string[1:-1]\n",
    "        formatted_list = formatted_string.split(',')\n",
    "        game['developer'] = formatted_list\n",
    "    except :\n",
    "        game['developer'] = None\n",
    "\n",
    "    # The publisher name is also at a fixed location. Not every game has a publisher, though.\n",
    "    try :\n",
    "        raw_string = code_block.find('div', class_='grid_label', string='Publisher').find_next('a').get_text()\n",
    "        formatted_list = raw_string.split(',')\n",
    "        game['publisher'] = formatted_list\n",
    "    except :\n",
    "        game['publisher'] = None\n",
    "\n",
    "    # Descriptions are at a fixed location.\n",
    "    try:\n",
    "        game['description'] = current_page_soup.find('meta', attrs={'name':'Description'}).get('content')\n",
    "    except :\n",
    "        game['description'] = 'Failed'\n",
    "\n",
    "    # The languages are listed as rows of a table.\n",
    "    # There are three different ways languages can be implemented in the game.\n",
    "    # As we look through the table, we'll store the languages in separate lists.\n",
    "    interface_languages = []\n",
    "    full_audio_languages = []\n",
    "    subtitles_languages = []\n",
    "    language_types = [interface_languages, full_audio_languages, subtitles_languages]\n",
    "\n",
    "    # The source code is compex so let's isolate the relevant block for safety.\n",
    "    try :\n",
    "        languages_code_block = current_page_soup.find('table', class_='game_language_options')\n",
    "    # I'll leave a note for myself to help with bugfixing if needed.\n",
    "    except :\n",
    "        language_types[0] = 'Did not find code block'\n",
    "        \n",
    "    # Each \"row\" of the table is separated by a re tag. However, there's an extra\n",
    "    # tr tag at the beginning of languages_code_block that I couldn't find a better\n",
    "    # way to work around - since it has no text, it'll throw an error on .get_text,\n",
    "    # so we can just try/except our way out of it.\n",
    "    try :\n",
    "        for row in languages_code_block.find_all('tr', class_='') :\n",
    "            try :\n",
    "                current_language = row.find('td', class_='ellipsis').get_text()\n",
    "                # The text has a lot of formatting in it. No more!\n",
    "                current_language = re.sub('\\t|\\n|\\r', '', current_language)\n",
    "\n",
    "                # The code block represents each cell of the row with a td class='checkcol'\n",
    "                # tag. In order, the three cells of each row are interface, full audio,\n",
    "                # and subtitles. If the language of that row does not have one of those\n",
    "                # services, then there will be no more code inside the tags. If it does,\n",
    "                # then there will be a \"span\" tag in there along with a checkmark.\n",
    "\n",
    "                # Since the three types of language services are always in order,\n",
    "                # we can basically use 'counter' to iterate through the list of lists\n",
    "                # of language service types and only append the name of the language\n",
    "                # if that section of code has the \"span\" tag that indicates a checkmark.\n",
    "                counter = 0\n",
    "                for column in row.find_all('td', class_='checkcol') :\n",
    "                    if column.find('span') :\n",
    "                        language_types[counter].append(current_language)\n",
    "                    counter += 1\n",
    "            except :\n",
    "                pass\n",
    "    # For bugfixing.\n",
    "    except :\n",
    "        language_types[0] = 'Found code block, failed to parse within code block'\n",
    "\n",
    "    # The full list of tags by name (instead of code) is in a different part of the page\n",
    "    # and requires a new code block.\n",
    "    tag_names_code_block = current_page_soup.find('div', attrs={'class':'glance_tags popular_tags'})\n",
    "    tag_names_list = []\n",
    "    try : \n",
    "        for tag_section in tag_names_code_block.find_all('a', class_='app_tag') :\n",
    "            tag_name = tag_section.get_text().strip()\n",
    "            tag_names_list.append(tag_name)\n",
    "        game['tag_list'] = tag_names_list.copy()\n",
    "    except :\n",
    "        game['tag_list'] = tag_names_list.copy()\n",
    "\n",
    "    # Now we add the lists to our dictionary. We can access the lists via\n",
    "    # the index of the language_types list of lists.\n",
    "    game['interface_languages'] = language_types[0]\n",
    "    game['full_audio_languages'] = language_types[1]\n",
    "    game['subtitles_languages'] = language_types[2]\n",
    "\n",
    "    # I would love to have rating data available for the games, but Steam does not\n",
    "    # present it systematically (probably because so many games are not rated,\n",
    "    # and because there are different rating systems.)\n",
    "    # Maybe someday.\n",
    "    # game['rating'] = PG, Mature Audiences, etc...\n",
    "\n",
    "    # Now we get the number of reviews that are in English.\n",
    "    # To get the numbers for other languages, we'll have to modify the URL parameters\n",
    "    # and get the page again, so that'll be a big ol'loop that we'll do later.\n",
    "    try:\n",
    "        raw_english_comments = current_page_soup.find('label', attrs={'for':'review_language_mine'}) \\\n",
    "                                                    .find_next('span', class_='user_reviews_count').get_text()\n",
    "        formatted_english_comments = int(raw_english_comments.replace(',', '').strip('()'))\n",
    "        game['english'] = formatted_english_comments\n",
    "                                                            \n",
    "    except:\n",
    "        game['english'] = 0\n",
    "\n",
    "    # Rinse and repeat.\n",
    "    games_extend_list.append(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm declaring/cleaning out the list in a different cell because I hit a lot of \n",
    "# exceptions while testing this, and I didn't want to accidentally clean out all\n",
    "# my previous hard work each time I made a fix and continued the process. \n",
    "games_extend_list = []\n",
    "\n",
    "# Since running the following cell requires repeated get requests and sleep intervals,\n",
    "# and since many failures tend to happen 20 minutes or more into the process,\n",
    "# we can build in a ticker that keeps track of how far we got LAST time.\n",
    "# Then, after we debug, we can start right over from where we left off. \n",
    "ticker = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping individual game page data...\n",
      "Scraped 2 game pages.\n"
     ]
    }
   ],
   "source": [
    "# For bugfixing.\n",
    "touched_ids = []\n",
    "\n",
    "print(\"Scraping individual game page data...\")\n",
    "\n",
    "# Now we loop over all all app_ids in the df we created earlier.\n",
    "for index, row in scraped_search_results_df.iterrows() :\n",
    "    \n",
    "    # This is for bugfixing. If the loop throws an exception, I can use the ticker\n",
    "    # variable to quickly pick up where we left off.\n",
    "    if index == ticker :\n",
    "        # Soup up the page.\n",
    "        url = row['game_page_link']\n",
    "        html = urlopen(url)\n",
    "        current_page_soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # Scrape the page.\n",
    "        scrape_game_page_data(current_page_soup, row['app_id'])\n",
    "\n",
    "        # Include a random delay to prevent getting IP blocked.\n",
    "        interval = 0.5 + random.random() * 0.3\n",
    "        time.sleep(interval)\n",
    "        \n",
    "        # If the loop throws an exception on a game, 'ticker' will thus be equal\n",
    "        # to that game's index in the df, and I can go see what the problem was.\n",
    "        ticker = index + 1\n",
    "        \n",
    "\n",
    "# Turn the new list of dicts into a new df.\n",
    "scraped_game_pages_df = pd.DataFrame(games_extend_list)\n",
    "scraped_game_pages_df.to_json('../data/raw/temp/Scraped Game Pages.json', orient='records')\n",
    "\n",
    "print(f\"Scraped {len(games_extend_list)} game pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we join our dataframes to create our core dataset.\n",
    "# I say \"core,\" even though our all-important label has yet to be scraped.\n",
    "# Bear with me. I'm new at this.\n",
    "joined_games_df = pd.merge(scraped_search_results_df, scraped_game_pages_df, on=\"app_id\", how='inner')\n",
    "joined_games_df.to_json('../data/raw/temp/Joined Games DF.json', orient='records')\n",
    "# joined_games_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Scrape the number of comments in each language from the games' pages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we begin the task of getting all the comment counts for each different language.\n",
    "# Since this process requires a huge amount of get requests/time, we'll limit our exploration\n",
    "# to the 10 most common languages for game localization (assuming the source text is English).\n",
    "\n",
    "# Here's a list of all the language codes on Steam, for good measure.\n",
    "# Don't know if we'll use it, but here it is.\n",
    "all_languages = ['schinese', 'tchinese', 'japanese', 'koreana', 'thai', 'bulgarian', 'czech', 'danish', \\\n",
    "                 'german', 'english', 'spanish', 'latam', 'greek', 'french', 'italian', 'indonesian', \\\n",
    "                 'hungarian', 'dutch', 'norwegian', 'polish', 'portugese', 'brazilian', 'romanian', \\\n",
    "                 'russian', 'finnish', 'swedish', 'turkish', 'vietnamese', 'ukranian']\n",
    "\n",
    "# For some languages, steam displays 'comments in my language' as including English. Let's make a list\n",
    "# of them for reference.\n",
    "# We will NOT subtract EN counts from these language comment counts for now, since the behavior of that was\n",
    "# wonky.\n",
    "# Since we're worried about ratio of comments normally to comments on this one game, this is not a HUGE\n",
    "# problem.... but I'll need to find a better solution for this at some point.\n",
    "languages_counted_with_english = ['german', 'danish', 'greek', 'dutch', 'norwegian', 'finnish', 'swedish']\n",
    "\n",
    "# Since we already have the EN comment counts, let's make a list that excludes EN for future\n",
    "# scraping. We can also remove all the languages that don't have their own distinct counts.\n",
    "all_counted_non_english_languages = all_languages.copy()\n",
    "all_counted_non_english_languages.remove('english')\n",
    "all_counted_non_english_languages.remove('portugese')\n",
    "all_counted_non_english_languages.remove('ukranian')\n",
    "\n",
    "# These are the generally-accepted top 10 languages to localize into from EN.\n",
    "# The count of EN comments is important for our analysis, but it's already in the df.\n",
    "# No idea why they put an a on the end of Korean.\n",
    "top_10_languages = ['german', 'french', 'spanish', 'brazilian', 'russian', 'italian', 'schinese', \\\n",
    "                    'japanese', 'koreana', 'polish']\n",
    "\n",
    "# %store all_languages\n",
    "# %store top_10_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build a function that will find the number of reviews in a given language for a given game.\n",
    "# This function will iterate through our df (using the first one, which is also the smallest, for\n",
    "# good measure), creating a new column for the language and filling the value with the number.\n",
    "app_comment_languages = []\n",
    "single_app_comment_languages = {}\n",
    "\n",
    "# We'll need this later. Just trust me.\n",
    "en_comment_counts_by_app_id = scraped_game_pages_df.set_index('app_id')['english']\n",
    "\n",
    "\n",
    "def comments_in_all_languages(app_id, languages) :\n",
    "    \"\"\"\n",
    "    Takes a Steam app id and a list of languages (as spelled in Steam's html)\n",
    "    and creates a dictionary, then appends that dictionary to a list.\n",
    "\n",
    "    Intended to be iterated over.\n",
    "\n",
    "    The first key in the dictionary is \"app id\", and the value is the app id.\n",
    "\n",
    "    The rest of the keys are the names of the languages, and the values are\n",
    "    the number of comments on that game/app's page that are in that language.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure the dict is empty at the beginning of each loop.\n",
    "    single_app_comment_languages = {}\n",
    "    \n",
    "    # Store the app_id in the dict.\n",
    "    single_app_comment_languages['app_id'] = app_id\n",
    "\n",
    "    # Soup up the game's page in the current language.\n",
    "    for language in languages :\n",
    "        url = 'https://store.steampowered.com/app/'+str(app_id)+'/?l='+language\n",
    "        html = urlopen(url)\n",
    "        current_page_soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # There are 2 types of game page source code, used on games with different language settings.\n",
    "        # We'll try the most common one first, then try to execute the other type if this throws an exception.\n",
    "        try :\n",
    "            raw_comment_count = current_page_soup.find('label', attrs={'for':'review_language_mine'}).find_next('span').get_text()\n",
    "            formatted_comment_count = int(raw_comment_count.replace(',', '').strip('()'))\n",
    "            single_app_comment_languages[language] = formatted_comment_count\n",
    "        \n",
    "        # If that's no good, we try scraping the other way.\n",
    "        # The 'other way' can't be scraped effectively by urlopen(), so we'll use requests.get() instead.\n",
    "        except :\n",
    "            try :\n",
    "                url = 'https://store.steampowered.com/app/'+str(app_id)+'/?l='+language\n",
    "                html = requests.get(url)\n",
    "                html_string = str(html.content)\n",
    "                raw_comment_count = re.split('<span class=\"user_reviews_count\">|</span> <a class=\"tooltip\" data-tooltip-html=', html_string)[-2]\n",
    "                formatted_comment_count = int(raw_comment_count.replace(',', '').strip('()'))\n",
    "                single_app_comment_languages[language] = formatted_comment_count\n",
    "            # If both fail, then it's a loss.\n",
    "            except:\n",
    "                single_app_comment_languages[language] = np.nan\n",
    "        \n",
    "        # Additional cleaning...\n",
    "        try :\n",
    "            # If the code block doens't parse, the var ends up null.\n",
    "            # Postgres disapproves.\n",
    "            if single_app_comment_languages[language] == None :\n",
    "                single_app_comment_languages[language] = 0\n",
    "        except :\n",
    "            # If something else went wrong, I need to know.\n",
    "            # This will cause the table to fail to ingest, so I can check.\n",
    "            single_app_comment_languages[language] = 'Failed_2'\n",
    "\n",
    "    # Rinse and repeat.\n",
    "    app_comment_languages.append(single_app_comment_languages)\n",
    "\n",
    "    interval = 0.5 + random.random() * 0.3\n",
    "    time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comment counts. This might take a while...\n",
      "Scraped 2 games' comment counts.\n",
      "Next scrape starts from https://store.steampowered.com/search/?sort_by=&sort_order=0&category1=998&supportedlang=english&page=873\n"
     ]
    }
   ],
   "source": [
    "# Now we iterate over that function for all app ids.\n",
    "# I'm also resetting the dic/list variables here since I ran these cells out of order a lot\n",
    "# during bugfixing.\n",
    "app_comment_languages = []\n",
    "single_app_comment_languages = {}\n",
    "\n",
    "# Pass each app_id into the function along with our list of target languages.\n",
    "\n",
    "print(f\"Scraping comment counts. This might take a while...\")\n",
    "\n",
    "try :\n",
    "    for index, row in scraped_search_results_df.iterrows() :\n",
    "        comments_in_all_languages(row['app_id'], top_10_languages)\n",
    "except Exception as e :\n",
    "    print(e)\n",
    "\n",
    "# Export because I'm risk-averse.\n",
    "comment_languages_df = pd.DataFrame(app_comment_languages)\n",
    "comment_languages_df.to_json('../data/raw/temp/Comment Languages DF.json', orient='records')\n",
    "\n",
    "print(f\"Scraped {len(app_comment_languages)} games' comment counts.\")\n",
    "print(f\"Next scrape starts from {next_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Save and Quit\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we merge our separate dfs.\n",
    "games_df = pd.merge(joined_games_df, comment_languages_df, on=\"app_id\", how='inner')\n",
    "\n",
    "# If we have existing records on disk, we add to them.\n",
    "if os.path.exists('../data/raw/0 - Scraped Games DF.pkl') :\n",
    "\n",
    "    with open('../data/raw/0 - Scraped Games DF.pkl', 'rb') as file:\n",
    "        existing_records = pd.read_pickle(file)\n",
    "    \n",
    "    brand_new_fancy_updated_version = pd.concat([existing_records, games_df], axis=0, ignore_index=True)\n",
    "    \n",
    "    with open('../data/raw/0 - Scraped Games DF.pkl', 'wb') as file:\n",
    "        brand_new_fancy_updated_version.to_pickle(file)\n",
    "    \n",
    "# If not, we begin the records on disk.\n",
    "else :\n",
    "    with open('../data/raw/0 - Scraped Games DF.pkl', 'wb+') as file:\n",
    "        pickle.dump(games_df, file)\n",
    "\n",
    "# Data scraped!\n",
    "        \n",
    "end_time = time.time()\n",
    "\n",
    "total_runtime = end_time - start_time\n",
    "\n",
    "seconds = int(total_runtime % 60)\n",
    "minutes = int(total_runtime // 60)\n",
    "hours = int(total_runtime // (60**2))\n",
    "\n",
    "# print(f\"{hours}h, {minutes}m, {seconds}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
