{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visits the 'recommendations' page of each user's profile and logs records with the following:\n",
    "1. 'user' ---- user's id\n",
    "2. 'app_id' ---- game's appid\n",
    "3. 'positive' ---- 1 if review is positive, 0 if negative\n",
    "4. 'total_playtime' ---- user's playtime in the game\n",
    "5. 'review_playtime' ---- in the game at the time of review, if different\n",
    "6. 'text' ---- text of review, if any\n",
    "7. 'helpful_count' ---- how many people found the review helpful\n",
    "8. 'review_date' ---- date of the review\n",
    "9. 'edit_date' ---- date the review was last edited, if any\n",
    "10. 'date_scraped' ---- date scraped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DS stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# String manipulation\n",
    "import re\n",
    "\n",
    "# Web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# For labeling records, tracking files, and formatting\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# For Rick\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # NOTE: Only use once in your life. Instantiates local holder files.\n",
    "\n",
    "# scraped_users = set([0])\n",
    "# with open('../data/raw/review_scraped_users.pkl', 'wb+') as file :\n",
    "#     pickle.dump(scraped_users, file)\n",
    "\n",
    "# skipped_users = {0:'hello world'}\n",
    "# with open('../data/raw/review_skipped_users.pkl', 'wb+') as file :\n",
    "#     pickle.dump(skipped_users, file)\n",
    "\n",
    "# users_with_no_reviews = set([0])\n",
    "# with open('../data/raw/users_with_no_reviews.pkl', 'wb+') as file :\n",
    "#     pickle.dump(users_with_no_reviews, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "with open('../data/raw/all_users', 'rb+') as file :\n",
    "    all_users = set(pickle.load(file))\n",
    "\n",
    "with open('../data/raw/review_scraped_users.pkl', 'rb+') as file :\n",
    "    review_scraped_users = pickle.load(file)\n",
    "\n",
    "with open('../data/raw/users_with_no_reviews.pkl', 'rb+') as file :\n",
    "    users_with_no_reviews = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish vars\n",
    "\n",
    "# Use as url.format(userid)\n",
    "url = 'https://steamcommunity.com/profiles/{}/recommended/'\n",
    "\n",
    "unscraped_users = all_users - review_scraped_users\n",
    "\n",
    "skipped_users = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For error handling\n",
    "\n",
    "def log_failure(user, message) :\n",
    "    print(message)\n",
    "    skipped_users[user] = message\n",
    "    with open('../data/raw/review_skipped_users.pkl', 'wb+') as file :\n",
    "        pickle.dump(skipped_users, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "\n",
    "counter = 0\n",
    "holding_list = []\n",
    "temp_scraped_users = set()\n",
    "\n",
    "for user in unscraped_users :\n",
    "\n",
    "    # Yes soup for you!\n",
    "    try :\n",
    "        html = urlopen(url.format(user))\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        review_blocks = soup.find_all('div', class_='review_box')\n",
    "        if len(review_blocks) == 0 :\n",
    "            with open('../data/raw/users_with_no_reviews.pkl', 'rb+') as file :\n",
    "                users_with_no_reviews = pickle.load(file)\n",
    "            users_with_no_reviews.add(user)\n",
    "            with open('../data/raw/users_with_no_reviews.pkl', 'wb+') as file :\n",
    "                pickle.dump(users_with_no_reviews, file)\n",
    "            temp_scraped_users.add(user)\n",
    "            continue\n",
    "    except :\n",
    "        log_failure(user, 'Unable to parse review blocks.')\n",
    "        continue\n",
    "\n",
    "    for review_block in review_blocks :\n",
    "\n",
    "        holding_dict = {}\n",
    "\n",
    "        # Specify user\n",
    "        holding_dict['user'] = user\n",
    "\n",
    "        # Get appid\n",
    "        try :\n",
    "            app_link = review_block.find('a').get('href')\n",
    "            app_id = re.findall(r'\\d+', app_link)\n",
    "            holding_dict['app_id'] = int(app_id[0])\n",
    "        except :\n",
    "            holding_dict['app_id'] = 'Failed'\n",
    "        \n",
    "        # Get 'positive'\n",
    "        try :\n",
    "            r_or_n = review_block.find('div', class_='title').find('a').get_text()\n",
    "            if r_or_n[0] == 'R' :\n",
    "                holding_dict['positive'] = 1\n",
    "            else :\n",
    "                holding_dict['positive'] = 0\n",
    "        except :\n",
    "            holding_dict['positive'] = 'Failed'\n",
    "\n",
    "        # Get playtimes\n",
    "        try :\n",
    "            playtimes = review_block.find('div', class_='hours').get_text().replace(',', '')\n",
    "            pruned_playtimes = re.findall(r'[-+]?\\d*\\.?\\d+|\\d+', playtimes)\n",
    "            for i in range(len(pruned_playtimes)) :\n",
    "                pruned_playtimes[i] = float(pruned_playtimes[i])\n",
    "            holding_dict['total_playtime'] = pruned_playtimes[0]\n",
    "            if len(pruned_playtimes) > 1 :\n",
    "                holding_dict['review_playtime'] = pruned_playtimes[1]\n",
    "            else :\n",
    "                holding_dict['review_playtime'] = pruned_playtimes[0]\n",
    "        except :\n",
    "            holding_dict['total_playtime'] = 'Failed'\n",
    "            holding_dict['review_playtime'] = 'Failed'\n",
    "\n",
    "        # Get review text\n",
    "        try :\n",
    "            review_text = review_block.find('div', class_='content').get_text().strip()\n",
    "            holding_dict['text'] = review_text\n",
    "        except :\n",
    "            holding_dict['text'] = 'Failed'\n",
    "\n",
    "        # Get 'helpful' counts\n",
    "        try :\n",
    "            helpful_str = review_block.find('div', class_='header').get_text()\n",
    "            helpful_count = re.findall(r'\\d+', helpful_str)\n",
    "            if len(helpful_count) != 0 :\n",
    "                holding_dict['helpful_count'] = int(helpful_count[0])\n",
    "            else :\n",
    "                holding_dict['helpful_count'] = 0\n",
    "        except :\n",
    "            holding_dict['helpful_count'] = 'Failed'\n",
    "\n",
    "        # Get review date\n",
    "        try :\n",
    "            posted_text = review_block.find('div', class_='posted').get_text().strip().replace('.', '')\n",
    "            review_date_text = posted_text[7:]\n",
    "            # If the review was edited, this str will contain the original date,\n",
    "            # then a bunch of weird, un-strip()-able whitespace, then the edited date.\n",
    "            # Let's split these two dates.\n",
    "            if len(review_date_text) > 20 :\n",
    "                date_texts = review_date_text.split('Last edited')\n",
    "                date_texts[0] = date_texts[0][:-9]\n",
    "                date_texts[1] = date_texts[1][1:]\n",
    "            else :\n",
    "                date_texts = [review_date_text, review_date_text]\n",
    "            # Dates for reviews made in the current year do not include the year.\n",
    "            # For consistency, we can add it manually.\n",
    "            for i in range(2) :\n",
    "                if ',' not in date_texts[i] :\n",
    "                    year = datetime.now().year\n",
    "                    date_texts[i] = date_texts[i] + f\", {year}\"\n",
    "                date_texts[i] = datetime.strptime(date_texts[i], '%B %d, %Y')\n",
    "            holding_dict['review_date'] = date_texts[0].date()\n",
    "            holding_dict['edit_date'] = date_texts[1].date()\n",
    "        except :\n",
    "            holding_dict['review_date'] = 'Failed'\n",
    "            holding_dict['edit_date'] = 'Failed'\n",
    "\n",
    "        # Log the scraped date\n",
    "        holding_dict['date_scraped'] = datetime.now().date()\n",
    "\n",
    "        # Append the holding dict to the df at this level\n",
    "        holding_list.append(holding_dict.copy())\n",
    "    \n",
    "    temp_scraped_users.add(user)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    if counter % 50 == 0 :\n",
    "        # Save to disk\n",
    "        this_round_df = pd.DataFrame(holding_list)\n",
    "        this_round_table = pa.Table.from_pandas(this_round_df)\n",
    "        extant_table = pq.read_table('../data/raw/review_table.parquet')\n",
    "        total_table = pa.concat_tables([extant_table, this_round_table])\n",
    "        # Release the memory\n",
    "        extant_table = None\n",
    "        pq.write_table(total_table, '../data/raw/review_table.parquet')\n",
    "        # Store len and release memory\n",
    "        all_records = len(total_table)\n",
    "        total_table = None\n",
    "\n",
    "        with open('../data/raw/review_scraped_users.pkl', 'rb+') as file :\n",
    "            review_scraped_users = pickle.load(file)\n",
    "        review_scraped_users.update(temp_scraped_users)\n",
    "        with open('../data/raw/review_scraped_users.pkl', 'wb+') as file :\n",
    "            pickle.dump(review_scraped_users, file)\n",
    "\n",
    "        # Report\n",
    "        print(f'Scraped {len(temp_scraped_users)} users this round')\n",
    "        print(f'Got {len(holding_list)} reivews')\n",
    "        print(f'{all_records} reviews are now in the bag, so to speak')\n",
    "        print(datetime.now().replace(microsecond=0))\n",
    "        print('')\n",
    "\n",
    "        # Reset\n",
    "        temp_scraped_users = set()\n",
    "        holding_list = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
