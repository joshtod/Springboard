{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook pulls data from the local recommendation engine project pipeline between the cleaning step and the ML preprocessing step, then pipes it to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPICallError, ServiceUnavailable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thee_to_google(project=\"steam-recommendation-engine\", file_path=None, \\\n",
    "                       table_id=None, schema=None, \\\n",
    "                       format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON, skip=None,\n",
    "                       clustering_fields=None) :\n",
    "    \"\"\"\n",
    "    Uploads a file to Google BigQuery.  \n",
    "      \n",
    "    'project' is the BigQuery project name.  \n",
    "    'file_path' is the location of the file to be uploaded (must be jsonl).  \n",
    "    'table_id' is the id of the table to be created in BigQuery.  \n",
    "    'schema' is the schema of the table.  \n",
    "    'format' indicates the file format within 'bigquery.SourceFormat'.  \n",
    "    'skip' sets the number of rows to skip (only specify if uploading csv).  \n",
    "    'clustering_fields' accepts a list of fields and can be left blank.\n",
    "    \"\"\"\n",
    "\n",
    "    client = bigquery.Client(project=project)\n",
    "\n",
    "    if skip :\n",
    "        params = {\n",
    "            'source_format':format,\n",
    "            'write_disposition':bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "            'skip_leading_rows':skip,\n",
    "            'schema':schema,\n",
    "            'clustering_fields':clustering_fields\n",
    "        }\n",
    "    else :\n",
    "        params = {\n",
    "            'source_format':format,\n",
    "            'write_disposition':bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "            'schema':schema,\n",
    "            'clustering_fields':clustering_fields\n",
    "        }\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(**params)\n",
    "\n",
    "    with open(file_path, 'rb') as source_file:\n",
    "        job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "\n",
    "    # Error handling.\n",
    "    attempts = 1\n",
    "    while attempts < 6 :\n",
    "        try :\n",
    "            job.result()\n",
    "            print(f'{job.output_rows} rows successfully loaded as {table_id}.')\n",
    "            break\n",
    "        except (GoogleAPICallError, ServiceUnavailable) as e:\n",
    "            print(f\"Fail {attempts}: {e}\")\n",
    "            time.sleep(attempts**2)\n",
    "            attempts += 1\n",
    "    else :\n",
    "        print(f\"All {attempts} attempts failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Games Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the columns we'll need for Google.\n",
    "new_column_order = ['app_id', 'title', 'developer', 'publisher', 'description', 'release_date', \\\n",
    "                    'price', 'number_of_reviews', 'positive_review_percent', 'tags', 'tag_list', \\\n",
    "                    'interface_languages', 'full_audio_languages', 'subtitles_languages', \\\n",
    "                    'game_page_link', 'date_scraped']\n",
    "\n",
    "# Load and create.\n",
    "games_df = pd.read_pickle('../data/interim/1 - Games DF - Wrangled.pkl')\n",
    "google_df = games_df.reindex(columns=new_column_order)\n",
    "\n",
    "# Prep the data specifically for BigQuery.\n",
    "for index, row in google_df.iterrows() :\n",
    "    if type(row['release_date']) == str :\n",
    "        google_df.at[index, 'release_date'] = None\n",
    "    if type(row['interface_languages']) == list and len(row['interface_languages']) > 1:\n",
    "        if len(row['interface_languages'][0]) == None :\n",
    "            google_df.at[index, 'interface_languages'] = []\n",
    "    if pd.isna(row['price']) :\n",
    "        google_df.at[index, 'price'] = 0.0\n",
    "\n",
    "google_df['release_date'] = pd.to_datetime(google_df['release_date']).dt.strftime('%Y-%m-%d')\n",
    "# google_df['price'] = google_df['price'].astype(int)\n",
    "google_df['number_of_reviews'] = google_df['number_of_reviews'].fillna(0)\n",
    "\n",
    "# Further simplification.\n",
    "google_df['positive_review_percent'] = (google_df['positive_review_percent'] * 100)\\\n",
    "    .fillna(0).astype(int)\n",
    "\n",
    "# Save locally before uploading.\n",
    "google_df.to_parquet('../data/interim/games_df_for_google.parquet', index=False)\n",
    "google_df.to_json('../data/interim/games_df_for_google.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100892 rows successfully loaded as steam-recommendation-engine.steam_data.games.\n"
     ]
    }
   ],
   "source": [
    "# For \"games\" table, stablish schemas and upload to BigQuery.\n",
    "file_path = '../data/interim/games_df_for_google.jsonl'\n",
    "table_id = 'steam-recommendation-engine.steam_data.games'\n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"app_id\", bigquery.enums.SqlTypeNames.INTEGER, mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"title\", bigquery.enums.SqlTypeNames.STRING, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"developer\", bigquery.enums.SqlTypeNames.STRING, mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"publisher\", bigquery.enums.SqlTypeNames.STRING, mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"description\", bigquery.enums.SqlTypeNames.STRING, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"release_date\", bigquery.enums.SqlTypeNames.DATE, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"price\", bigquery.enums.SqlTypeNames.FLOAT, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"number_of_reviews\", bigquery.enums.SqlTypeNames.FLOAT, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"positive_review_percent\", bigquery.enums.SqlTypeNames.FLOAT, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tags\", bigquery.enums.SqlTypeNames.STRING, mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"tag_list\", bigquery.enums.SqlTypeNames.STRING, mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"interface_languages\", bigquery.enums.SqlTypeNames.STRING, mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"full_audio_languages\", bigquery.enums.SqlTypeNames.STRING, mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"subtitles_languages\", bigquery.enums.SqlTypeNames.STRING, mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"game_page_link\", bigquery.enums.SqlTypeNames.STRING, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"date_scraped\", bigquery.enums.SqlTypeNames.DATE, mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "clustering_fields = ['release_date']\n",
    "\n",
    "get_thee_to_google(file_path=file_path, table_id=table_id, \\\n",
    "                   schema=schema, clustering_fields=clustering_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the set of all user ids\n",
    "with open('../data/raw/all_users', 'rb+') as file :\n",
    "    all_users = set(pickle.load(file))\n",
    "\n",
    "# Tableify it\n",
    "all_users_table = pd.DataFrame(all_users)\n",
    "all_users_table.columns = ['user_id']\n",
    "\n",
    "# Change id from str to int64 to save storage space\n",
    "all_users_table['user_id'] = pd.to_numeric(all_users_table['user_id'], errors='coerce')\n",
    "\n",
    "# Save to disk\n",
    "all_users_table.to_json('../data/interim/all_users_table_for_google.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5069512 rows successfully loaded as steam-recommendation-engine.steam_data.users.\n"
     ]
    }
   ],
   "source": [
    "# For \"users\" table, stablish schemas and upload to BigQuery.\n",
    "file_path = '../data/interim/all_users_table_for_google.jsonl'\n",
    "table_id = 'steam-recommendation-engine.steam_data.users'\n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"user_id\", bigquery.enums.SqlTypeNames.INTEGER, mode=\"REQUIRED\"),\n",
    "]\n",
    "\n",
    "get_thee_to_google(file_path=file_path, table_id=table_id, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_table = pd.read_parquet('../data/interim/cleaned_reviews.parquet')\n",
    "\n",
    "# For consistency and readability\n",
    "review_table.rename(columns={'user': 'user_id', 'total_playtime':'hours_played_total', \\\n",
    "                             'review_playtime':'hours_played_at_review'}, inplace=True)\n",
    "\n",
    "# Dates need finessing for Google to accept\n",
    "review_table['review_date'] = pd.to_datetime(review_table['review_date']).dt.strftime('%Y-%m-%d')\n",
    "review_table['edit_date'] = pd.to_datetime(review_table['edit_date']).dt.strftime('%Y-%m-%d')\n",
    "review_table['date_scraped'] = pd.to_datetime(review_table['date_scraped']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Save locally\n",
    "review_table.to_json('../data/interim/review_table_for_google.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6747619 rows successfully loaded as steam-recommendation-engine.steam_data.reviews.\n"
     ]
    }
   ],
   "source": [
    "# For \"reviews\" table, stablish schemas and upload to BigQuery.\n",
    "file_path = '../data/interim/review_table_for_google.jsonl'\n",
    "table_id = 'steam-recommendation-engine.steam_data.reviews'\n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"user_id\", bigquery.enums.SqlTypeNames.INTEGER, mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"app_id\", bigquery.enums.SqlTypeNames.INTEGER, mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"positive\", bigquery.enums.SqlTypeNames.BOOL, mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"hours_played_total\", bigquery.enums.SqlTypeNames.FLOAT, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"hours_played_at_review\", bigquery.enums.SqlTypeNames.FLOAT, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"text\", bigquery.enums.SqlTypeNames.STRING, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"helpful_count\", bigquery.enums.SqlTypeNames.INTEGER, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"review_date\", bigquery.enums.SqlTypeNames.DATE, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"edit_date\", bigquery.enums.SqlTypeNames.DATE, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"date_scraped\", bigquery.enums.SqlTypeNames.DATE, mode=\"NULLABLE\")\n",
    "]\n",
    "\n",
    "clustering_fields = ['app_id', 'review_date']\n",
    "\n",
    "get_thee_to_google(file_path=file_path, table_id=table_id, \\\n",
    "                   schema=schema, clustering_fields=clustering_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships_table = pd.read_parquet('../data/interim/all_relationships_cleaned.parquet')\n",
    "\n",
    "# Dates need finessing for Google to accept\n",
    "relationships_table['friend_since'] = pd.to_datetime(relationships_table['friend_since']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Save locally\n",
    "relationships_table.to_json('../data/interim/relationships_table_for_google.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072233 rows successfully loaded as steam-recommendation-engine.steam_data.relationships.\n"
     ]
    }
   ],
   "source": [
    "# For \"relationships\" table, stablish schemas and upload to BigQuery.\n",
    "file_path = '../data/interim/relationships_table_for_google.jsonl'\n",
    "table_id = 'steam-recommendation-engine.steam_data.relationships'\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"users\", bigquery.enums.SqlTypeNames.INTEGER, mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"friend_since\", bigquery.enums.SqlTypeNames.DATE, mode=\"NULLABLE\")\n",
    "]\n",
    "\n",
    "get_thee_to_google(file_path=file_path, table_id=table_id, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recently Played Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "recently_played_table = pd.read_pickle('../data/interim/recently_played_cleaned.pkl')\n",
    "\n",
    "# For consistency and readability\n",
    "recently_played_table.rename(columns={'user':'user_id', 'playtime_2w':'hours_played_last_2_weeks', \\\n",
    "                                      'playtime_f':'hours_played_total'}, inplace=True)\n",
    "\n",
    "# Save locally. Doing this one as CSV because jsonl gave odd \"row too long\" errors.\n",
    "recently_played_table.to_csv('../data/interim/recently_played_table_for_google.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4181501 rows successfully loaded as steam-recommendation-engine.steam_data.recently_played.\n"
     ]
    }
   ],
   "source": [
    "# For \"recently_played\" table, stablish schemas and upload to BigQuery.\n",
    "file_path = '../data/interim/recently_played_table_for_google.csv'\n",
    "table_id = 'steam-recommendation-engine.steam_data.recently_played'\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"user_id\", bigquery.enums.SqlTypeNames.INTEGER, mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"app_id\", bigquery.enums.SqlTypeNames.INTEGER, mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"hours_played_in_last_2_weeks\", bigquery.enums.SqlTypeNames.INTEGER, mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"hours_played_total\", bigquery.enums.SqlTypeNames.INTEGER, mode=\"NULLABLE\")\n",
    "]\n",
    "\n",
    "clustering_fields = ['user_id']\n",
    "\n",
    "get_thee_to_google(file_path=file_path, table_id=table_id, \\\n",
    "                   schema=schema, clustering_fields=clustering_fields, \\\n",
    "                   format=bigquery.SourceFormat.CSV, skip=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Company Duplicates (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed potential duplicates: 3416\n",
      "Unsuccessfully parsed potential duplicates: 12\n"
     ]
    }
   ],
   "source": [
    "# These potential duplicates were identified using fuzzywuzzy with a\n",
    "# threshold of 90. Verifying duplicates, selecting canonical forms,\n",
    "# and performing replacement will take more time than I have at the\n",
    "# moment, so I'll just add this file to the dataset for later.\n",
    "\n",
    "pot_dups = set()\n",
    "bad_dups = set()\n",
    "\n",
    "with open('../data/interim/potential_company_duplicates.txt', 'r') as file :\n",
    "    for line in file :\n",
    "        cleaned = line.replace('\\n', '')\n",
    "        tup = tuple(cleaned.split(' - '))\n",
    "        if len(tup) == 2 :\n",
    "            pot_dups.add(tup)        \n",
    "        else :\n",
    "            bad_dups.add(tup)    \n",
    "\n",
    "print(f\"Successfully parsed potential duplicates: {len(pot_dups)}\")\n",
    "print(f\"Unsuccessfully parsed potential duplicates: {len(bad_dups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('BYU Animation', ' Class of 2017', 'BYU Animation', ' Class of 2016'),\n",
       " ('BYU Animation', ' Class of 2018', 'BYU Animation', ' Class of 2016'),\n",
       " ('BYU Animation', ' Class of 2018', 'BYU Animation', ' Class of 2017'),\n",
       " ('BYU Animation', 'Class of 2023', 'BYU Animation', ' Class of 2016'),\n",
       " ('BYU Animation', 'Class of 2023', 'BYU Animation', ' Class of 2017'),\n",
       " ('BYU Animation', 'Class of 2023', 'BYU Animation', ' Class of 2018'),\n",
       " ('Chris Bell',\n",
       "  'Creative Design Studios',\n",
       "  'Chris Bell – Creative Design Studios'),\n",
       " ('Dovetail Games', 'Trains', 'Dovetail Games ', 'Trains'),\n",
       " ('Dovetail Games', 'Trains', 'Dovetail Games – Trains'),\n",
       " ('Dovetail Games ', 'Trains', 'Dovetail Games – Trains'),\n",
       " ('Dreams of Heaven', 'Games', 'Dreams of Heaven Games'),\n",
       " ('Ubisoft', 'San Francisco', 'Ubisoft San Francisco')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lookie lookie\n",
    "bad_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3422\n"
     ]
    }
   ],
   "source": [
    "# Looks like the first 6 of those are not real potential duplicates.\n",
    "# Let's manually handle the remaining 6.\n",
    "# Gotta be careful with those en dashes and extra spaces...\n",
    "\n",
    "fixeds = [\n",
    "    ('Chris Bell - Creative Design Studios', 'Chris Bell – Creative Design Studios'),\n",
    "    ('Dovetail Games - Trains', 'Dovetail Games  - Trains'),\n",
    "    ('Dovetail Games - Trains', 'Dovetail Games – Trains'),\n",
    "    ('Dovetail Games  - Trains', 'Dovetail Games – Trains'),\n",
    "    ('Dreams of Heaven - Games', 'Dreams of Heaven Games'),\n",
    "    ('Ubisoft - San Francisco', 'Ubisoft San Francisco')\n",
    "]\n",
    "\n",
    "for item in fixeds :\n",
    "    pot_dups.add(item)\n",
    "\n",
    "print(len(pot_dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The count is correct. Now let's save locally.\n",
    "potential_duplicates = pd.DataFrame(pot_dups)\n",
    "potential_duplicates.rename(columns={0:'op_1', 1:'op_2'}, inplace=True)\n",
    "\n",
    "potential_duplicates.to_json('../data/interim/potential_duplicates_for_google.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3422 rows successfully loaded as steam-recommendation-engine.steam_data.potential_duplicates.\n"
     ]
    }
   ],
   "source": [
    "# And a one, and a two, and a you-know-what-to-do...\n",
    "file_path = '../data/interim/potential_duplicates_for_google.jsonl'\n",
    "table_id = 'steam-recommendation-engine.steam_data.potential_duplicates'\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"op_1\", bigquery.enums.SqlTypeNames.STRING, mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"op_2\", bigquery.enums.SqlTypeNames.STRING, mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "get_thee_to_google(file_path=file_path, table_id=table_id, schema=schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
