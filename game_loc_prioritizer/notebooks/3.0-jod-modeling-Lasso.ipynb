{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Modeling!\n",
    "### Yee-Haw!\n",
    "\n",
    "As a control, I'm just gonna linearly regress right quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/2 - Games DF - PreProcessed Features', 'rb') as file :\n",
    "    X = pickle.load(file)\n",
    "\n",
    "with open('../data/processed/2 - Games DF - PreProcessed Targets', 'rb') as file :\n",
    "    y_universe = pickle.load(file)\n",
    "\n",
    "%store -r relevant_langs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're All Agnostics Now!\n",
    "---\n",
    "\n",
    "I'll choose the \"agnostic\" case for our initial analysis, and calculate basic vanilla lasso scores for all languages under that condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let me get a persistent set of indexes for my train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_universe, test_size=0.2)\n",
    "\n",
    "training_indexes = y_train.index\n",
    "testing_indexes = y_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_scores = {}\n",
    "\n",
    "for lang in relevant_langs :\n",
    "\n",
    "    # Create the target variable for this lang\n",
    "    y = {}\n",
    "    for index, row in y_universe.iterrows() :\n",
    "        y[index] = row['comment_diff_agnostic'][lang]\n",
    "    y = pd.Series(y)\n",
    "\n",
    "    # Generate our target splits based on the persistent train/test index split\n",
    "    y_train = y[training_indexes]\n",
    "    y_test = y[testing_indexes]\n",
    "\n",
    "    \n",
    "\n",
    "    # Run the model\n",
    "    model = Lasso()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Score it and log it\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    naive_scores[lang] = score\n",
    "    print(f\"{lang}: {round(score, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are weak, but the vast majority did better than chance. That might mean our idea is not complete bunk.\n",
    "\n",
    "Let's see if hyperparameter tuning helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the function we'll use to tune our alpha\n",
    "def Lasso_eval(alpha) :\n",
    "    global X\n",
    "    global y\n",
    "    params = {\"alpha\":alpha}\n",
    "    model = Lasso(max_iter=100000, **params)\n",
    "    score = cross_val_score(model, X, y, cv=5).mean()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, do run the model again, optimizing on each lang\n",
    "\n",
    "for lang in relevant_langs :\n",
    "\n",
    "    # Create the target variable for this lang\n",
    "    y = {}\n",
    "    for index, row in y_universe.iterrows() :\n",
    "        y[index] = row['comment_diff_agnostic'][lang]\n",
    "    y = pd.Series(y)\n",
    "\n",
    "    # Run some optimization\n",
    "    param_bounds = {\"alpha\":(0, 100)}\n",
    "    optimal = BayesianOptimization(Lasso_eval, param_bounds, verbose=False, allow_duplicate_points=True)\n",
    "    optimal.maximize(60, 60)\n",
    "    bayes_alpha = optimal.max['params']['alpha']\n",
    "\n",
    "    # Generate our target splits based on the persistent train/test index split\n",
    "    y_train = y[training_indexes]\n",
    "    y_test = y[testing_indexes]\n",
    "\n",
    "    # Run the model\n",
    "    model = Lasso(max_iter=100000, alpha=bayes_alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Score it and log it\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    rounded_score = round(score, 5)\n",
    "    improvement = round(naive_scores[lang]-score, 6)\n",
    "    rounded_alpha = round(optimal.max['params']['alpha'], 2)\n",
    "    print(f\"{lang}: {rounded_score} (improvement: {improvement}, alpha: {rounded_alpha})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope.\n",
    "\n",
    "I can only assume this is because the variability in the data makes the cv split scores so random that the tuned parameters end up essentially random as well.\n",
    "\n",
    "So that's one strike AGAINST our idea, though I'd argue that the uniformly weak positive of the naive parameter model indicates that we are onto *something*, no matter how slight.\n",
    "\n",
    "Perhaps the relationships between the tags can be better captured by other models. Let's try gradient boosting next, and then neural nets.\n",
    "\n",
    "Huzzah! Onward!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
