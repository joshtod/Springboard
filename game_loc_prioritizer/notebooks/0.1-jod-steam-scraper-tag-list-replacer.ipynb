{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag list replacer\n",
    "\n",
    "When I initially scraped records (17,000+ records extant at the time of this notebook's creation, scraping at a rate of 200 games/hr), I only pulled the 7 most common tags from each game. Those were easier to get from the Steam store, since Steam displays them in a convenient location for scraping.\n",
    "\n",
    "However, once I had enough records to begin analysis, I discovered that this made my feature space too sparse. Fewer than half of the records had 2 or more usable tags, and that's with a very generous definition of \"useful\" (specifically, \"appears in >5% of records\", which is super sparse in its own right).\n",
    "\n",
    "To prevent scraping from scratch, we now have to devise a way to grab an extended tag list for each game and use it to replace the tag list in our existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DS stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Trying not to get blocked while scraping by inputting\n",
    "# random delays between Get requests.\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# To help see if we have existing data or not.\n",
    "import os\n",
    "\n",
    "# For Rick\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary data.\n",
    "\n",
    "%store -r tags_dict\n",
    "\n",
    "with open('../data/raw/0.5 - Scraped Games DF with Tag Lists.pkl', 'rb') as file :\n",
    "    games_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of each iteration.\n",
    "iterations = 20\n",
    "counter = 0\n",
    "successful_indexes = []\n",
    "failed_indexes = []\n",
    "total_touched = []\n",
    "skipped_indexes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Step 1: Find the Full Tag List\n",
    "\n",
    "Turns out there's no tag-key list, but the full TEXT of each tag value does appear in a specific div:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete.\n",
      "Successfully updated: 20\n",
      "Failed to update: 0\n",
      "Total touched: 20\n",
      "Current index: 10339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in games_df.iterrows() :\n",
    "\n",
    "    # Check to see if we already have tags for this game.\n",
    "    if (row['tag_list'] != []) :\n",
    "        continue\n",
    "\n",
    "    # Try to call the page. Sometimes this fails randomly\n",
    "    try :\n",
    "        url = row['game_page_link']\n",
    "        html = urlopen(url)\n",
    "        current_page_soup = BeautifulSoup(html, 'lxml')\n",
    "    except :\n",
    "        print(\"Failed call. Retrying in 2 min...\")\n",
    "        skipped_indexes.append(index)\n",
    "        %store skipped_indexes\n",
    "        time.sleep(120)\n",
    "        continue\n",
    "\n",
    "    try :\n",
    "        code_block = current_page_soup.find('div', attrs={'class':'glance_tags popular_tags'})\n",
    "        successful_indexes.append(index)\n",
    "        counter += 1\n",
    "    except :\n",
    "        games_df.at[index, 'tag_list'] = ['Failed']\n",
    "        failed_indexes.append(index)\n",
    "        counter += 1\n",
    "        continue\n",
    "\n",
    "    tag_names_list = []\n",
    "\n",
    "    for tag_section in code_block.find_all('a', class_='app_tag') :\n",
    "        tag_name = tag_section.get_text().strip()\n",
    "        tag_names_list.append(tag_name)\n",
    "    \n",
    "    games_df.at[index, 'tag_list'] = tag_names_list.copy()\n",
    "\n",
    "    if counter == iterations :\n",
    "        print(f\"Batch complete.\")\n",
    "        print(f\"Successfully updated: {len(successful_indexes)}\")\n",
    "        print(f\"Failed to update: {len(failed_indexes)}\")\n",
    "        print(f\"Total touched: {len(successful_indexes) + len(failed_indexes)}\")\n",
    "        print(f\"Current index: {index}\")\n",
    "        print(f\"\")\n",
    "        counter = 0\n",
    "        successful_indexes = []\n",
    "        failed_indexes = []\n",
    "        with open('../data/raw/0.5 - Scraped Games DF with Tag Lists.pkl', 'wb') as file :\n",
    "            pickle.dump(games_df, file)\n",
    "    \n",
    "    delay = 0.3 + random.random() * 0.3\n",
    "    time.sleep(delay)\n",
    "\n",
    "print(\"COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
